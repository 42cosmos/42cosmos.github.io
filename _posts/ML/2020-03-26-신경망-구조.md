---
categories: ML/DL
title: '신경망 구조'
---

기본단위 : 뉴런 -> 조합해 복잡한 구조를 이룸

perceptron
노드는 뉴런의 연산을 표현 > 들어온 입력에 대한 가중치를 곱해 더해준 다음 > 액티베이션 펑션을 적용시킴

두 계층 사이 모든 뉴런 연결 -> 전결합계층 : 뉴럴 네트워크의 가장 기본이며, Dense Layer 라고도 칭함

뉴런 각 개를 하나의 백터로 묶어줌 :: 백터 내적을 묶어주면 Weight * x 로 표현이 가능함


### 얕은 신경망

은닉 Activation function으로 시그모이드 / 탄젠트를 주로 사용

출력 > Linear / Soft / Sigmoid ( Binary )


### 심층 신경망

5개 이상의 은닉 계층이 있는 경우 
sigmoid 대신 탄젠트 / relu 가 자주 사용됨



# 최적화 이론

### 분석적 방법 

수식을 알고 있을 때 > 미분해서 0이 된다는 것 : 기울기 0 -> 2차 미분 해서 0보다 크면 아래로 볼록한 모양

여기 다시 들어 보기



### 수치적 방법

함수 형태와 수식을 모를 때 

Gradient Method



전역 솔루션 : 전체에서 단 하나 -> 전체에서 가장 작은 값
지역 솔루션 : 여러 개 일 수 있고, 좁은 지역에서 봤을 때 솔루션이 되는 부분

하나의 솔루션이 전역인지 지역인지 알 수 없음. 

손실함수를 최소화 하는 방향으로 네트워크 파라미터 (웨이트-w, 편향-b) 를 조절하는 것 

