# TensorFlow

` import tensorflow as tf`   



### Perceptron 

인체 내 신경세포의 논리게이트 화 

여러 신호를 입력 받아 하나의 신호로 변경 > inputs

각 가중치 변경 (시냅스 - 연결의 가중치)

활성함수를 통한 논리적 변경 > outputs  - 비선형 변환

- 벡터 내적 : y = sign(x^T w) - 행 백터의 열벡터화 * 행 벡터 > 

사인 함수 - 입력의 사인이 플러스 ( 마이너스 )인지 확인



### 이진 분류 문제

#### Perceptron 동작

편향 표현 - 1 : 전체적 출력이 얼마나 shift 된 지 알아보게 함

학습률 - 부분 설명 다시 듣기





### 활성함수

#### Sign 함수의 문제

결정경계 ( Decision bounday )와의 거리를 신경써야 함



#### tahn 함수

- 입력값이 0에 가까울 수록 출력이 빠르게 변함
- 모든 점에서 미분 가능 : 그라디언트 디센트를 배울 때 중요함



#### Sigmoid func

1개의 입력

0에서 1사이 값을 가짐 - 확률 표현이 가능함 ( Binary classification 에 많이 사용됨 - T일 확률과 F (1 - T)일 확률) 

딥러닝에서는 0 센터의 0~1 값을 갖는 함수로만 사용됨



#### Softmax func

n개의 입력

각 입력의 지수함수를 정규화 함 ( 총 합이 1이 되게끔 함 )

e.g. 총 4가지 출력이 있을 때, 출력 0.2, 0.4, 0.1이 나왔다면 나머지는 자동으로 0.4가 됨



#### ReLU func

0보다 작은 값에 0 값을 강제함

딥러닝에서 가장 많이 사용되는 활성 함수 - 미분값이 일정해서 학습이 잘 됨 (0 or 1의 값이 나옴)

