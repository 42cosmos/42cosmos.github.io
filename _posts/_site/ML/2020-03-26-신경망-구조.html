<p>기본단위 : 뉴런 -&gt; 조합해 복잡한 구조를 이룸</p>

<p>perceptron
노드는 뉴런의 연산을 표현 &gt; 들어온 입력에 대한 가중치를 곱해 더해준 다음 &gt; 액티베이션 펑션을 적용시킴</p>

<p>두 계층 사이 모든 뉴런 연결 -&gt; 전결합계층 : 뉴럴 네트워크의 가장 기본이며, Dense Layer 라고도 칭함</p>

<p>뉴런 각 개를 하나의 백터로 묶어줌 :: 백터 내적을 묶어주면 Weight * x 로 표현이 가능함</p>

<h3 id="얕은-신경망">얕은 신경망</h3>

<p>은닉 Activation function으로 시그모이드 / 탄젠트를 주로 사용</p>

<p>출력 &gt; Linear / Soft / Sigmoid ( Binary )</p>

<h3 id="심층-신경망">심층 신경망</h3>

<p>5개 이상의 은닉 계층이 있는 경우 
sigmoid 대신 탄젠트 / relu 가 자주 사용됨</p>

<h1 id="최적화-이론">최적화 이론</h1>

<h3 id="분석적-방법">분석적 방법</h3>

<p>수식을 알고 있을 때 &gt; 미분해서 0이 된다는 것 : 기울기 0 -&gt; 2차 미분 해서 0보다 크면 아래로 볼록한 모양</p>

<p>여기 다시 들어 보기</p>

<h3 id="수치적-방법">수치적 방법</h3>

<p>함수 형태와 수식을 모를 때</p>

<p>Gradient Method</p>

<p>전역 솔루션 : 전체에서 단 하나 -&gt; 전체에서 가장 작은 값
지역 솔루션 : 여러 개 일 수 있고, 좁은 지역에서 봤을 때 솔루션이 되는 부분</p>

<p>하나의 솔루션이 전역인지 지역인지 알 수 없음.</p>

<p>손실함수를 최소화 하는 방향으로 네트워크 파라미터 (웨이트-w, 편향-b) 를 조절하는 것</p>

