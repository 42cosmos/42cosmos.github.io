<p><code class="language-plaintext highlighter-rouge"> import tensorflow as tf</code></p>

<h3 id="perceptron">Perceptron</h3>

<p>인체 내 신경세포의 논리게이트 화</p>

<p>여러 신호를 입력 받아 하나의 신호로 변경 &gt; inputs</p>

<p>각 가중치 변경 (시냅스 - 연결의 가중치)</p>

<p>활성함수를 통한 논리적 변경 &gt; outputs  - 비선형 변환</p>

<ul>
  <li>벡터 내적 : y = sign(x^T w) - 행 백터의 열벡터화 * 행 벡터 &gt;</li>
</ul>

<p>사인 함수 - 입력의 사인이 플러스 ( 마이너스 )인지 확인</p>

<h3 id="이진-분류-문제">이진 분류 문제</h3>

<h4 id="perceptron-동작">Perceptron 동작</h4>

<p>편향 표현 - 1 : 전체적 출력이 얼마나 shift 된 지 알아보게 함</p>

<p>학습률 - 부분 설명 다시 듣기</p>

<h3 id="활성함수">활성함수</h3>

<h4 id="sign-함수의-문제">Sign 함수의 문제</h4>

<p>결정경계 ( Decision bounday )와의 거리를 신경써야 함</p>

<h4 id="tahn-함수">tahn 함수</h4>

<ul>
  <li>입력값이 0에 가까울 수록 출력이 빠르게 변함</li>
  <li>모든 점에서 미분 가능 : 그라디언트 디센트를 배울 때 중요함</li>
</ul>

<h4 id="sigmoid-func">Sigmoid func</h4>

<p>1개의 입력</p>

<p>0에서 1사이 값을 가짐 - 확률 표현이 가능함 ( Binary classification 에 많이 사용됨 - T일 확률과 F (1 - T)일 확률)</p>

<p>딥러닝에서는 0 센터의 0~1 값을 갖는 함수로만 사용됨</p>

<h4 id="softmax-func">Softmax func</h4>

<p>n개의 입력<br />
각 입력의 지수함수를 정규화 함 ( 총 합이 1이 되게끔 함 )<br />
e.g. 총 4가지 출력이 있을 때, 출력 0.2, 0.4, 0.1이 나왔다면 나머지는 자동으로 0.4가 됨</p>

<h4 id="relu-func">ReLU func</h4>

<p>0보다 작은 값에 0 값을 강제함<br />
딥러닝에서 가장 많이 사용되는 활성 함수 - 미분값이 일정해서 학습이 잘 됨 (0 or 1의 값이 나옴)</p>

